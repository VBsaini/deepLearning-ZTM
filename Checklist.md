**Neural Networks (NN)**
=========================

### Introduction to Neural Networks

* [ ] Perceptron, Multi-Layer Perceptron (MLP)
* [ ] Activation Functions: Sigmoid, ReLU, Tanh
* [ ] Backpropagation and Gradient Descent
* [ ] Loss Functions: Cross-Entropy, Mean Squared Error (MSE)

### Training and Optimization

* [ ] Stochastic Gradient Descent (SGD), Adam Optimizer
* [ ] Learning rate, Batch size, Epochs
* [ ] Overfitting and Regularization (Dropout, L2 Regularization)

**Deep Learning (DNN)**
=========================

### Deep Neural Networks (DNN)

* [ ] Building and training deeper neural networks
* [ ] Weight Initialization Techniques
* [ ] Vanishing/Exploding Gradient problem and solutions (Batch Normalization, ReLU)

### Convolutional Neural Networks (CNNs)

* [ ] CNN Architecture: Convolutional Layers, Pooling Layers, Fully Connected Layers
* [ ] Applications: Image Classification, Object Detection

### Recurrent Neural Networks (RNNs)

* [ ] RNN Architecture: Sequence Processing, Backpropagation Through Time (BPTT)
* [ ] Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
* [ ] Applications: Time Series, Natural Language Processing